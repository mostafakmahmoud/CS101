INDEX:
A nested list of keywords and urls those keywords are found.
mylist = [
['key1',['url1','url2']],
['key2',['url3','url4']],
['key3',['url5','url6']]
]

HASHTABLE:
A Sample Nested list of buckets with keywords and urls.
htable = [
[['key1',['url','url','url']],['key2',['url','url','url']],['key3',['url','url']]],
[['key4',['url','url','url']],['key5',['url','url','url']],['key6',['url','url']]],
[['key7',['url','url','url']],['key8',['url','url','url']],['key9',['url','url']]],
]

get_all_links:
keeps calling get_next_target until no url left in the page.

get_next_target:
extracts the next url from a page
returns the url and where it stopped in that page.

get_page:
returns source code for a webpage.

add_to_index:
checks if a keyword is in the index.
if found, adds the url to list of urls for that keyword.
if not, adds a new keyword entry and url where it was found.

add_page_to_index (index, url, content):
splits the content into list of words.
updates the index to include all of the word occurances found in the
page content and adding urls to the word associated url list.

crawl_web:
takes a seed page url, calls get_all_links.
Stores the returned urls and calls get_all_links on them.
Keeps track of crawled pages to avoid repetition

lookup:
looks in the index for the keyword passed.
returns list of urls associated with that keyword.

hashtable_get_bucket:
takes a hashtable and keyword, returns that bucket where that keyword is.

make_hashtable:
takes a number of buckets, returns a hashtable containing that many buckets.

hash_string:
takes a keyword and number of buckets, returns the bucket number where that keyword should be.
